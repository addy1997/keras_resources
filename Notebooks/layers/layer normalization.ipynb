{ 
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-04928c4c383a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m tf.keras.layers.LayerNormalization(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "#layerNormalization\n",
    "#Method\n",
    "\n",
    "tf.keras.layers.LayerNormalization(\n",
    "    axis=-1,\n",
    "    epsilon=0.001,\n",
    "    center=True,\n",
    "    scale=True,\n",
    "    beta_initializer=\"zeros\",\n",
    "    gamma_initializer=\"ones\",\n",
    "    beta_regularizer=None,\n",
    "    gamma_regularizer=None,\n",
    "    beta_constraint=None,\n",
    "    gamma_constraint=None,\n",
    "    trainable=True,\n",
    "    name=None,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "#Notes\n",
    "\n",
    "\"\"\"\n",
    "1. this normalizes the activations of previous layer for each given sample in a batch (independently). \n",
    "2. the main difference between batch normalization and layer norm is that normalization across the batch not layer by layer.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LayerNormalization' from 'tensorflow.keras.layers' (/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/api/_v2/keras/layers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-938b7ca077f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayerNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LayerNormalization' from 'tensorflow.keras.layers' (/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/api/_v2/keras/layers/__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "\n",
    "data = tf.constant(np.arange(10).reshape(5, 2) * 10, dtype=tf.float32)\n",
    "\n",
    "print(data)\n",
    "layer = BatchNormalization(axis=1, scale =True)\n",
    "output= layer1(data)\n",
    "\n",
    "output\n",
    "\n",
    "\n",
    "#expected output\n",
    "\"\"\"\n",
    "tf.Tensor(\n",
    "[[-1. 1.]\n",
    " [-1. 1.]\n",
    " [-1. 1.]\n",
    " [-1. 1.]\n",
    " [-1. 1.]], shape=(5, 2), dtype=float32)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If scale or center are enabled, the layer will scale the normalized outputs by broadcasting them with a trainable variable gamma, and center the outputs by broadcasting with a trainable variable beta. gamma will default to a ones tensor and beta will default to a zeros tensor, so that centering and scaling are no-ops before training has begun.\n",
    "\n",
    "So, with scaling and centering enabled the normalization equations are as follows: Let the intermediate activations for a mini-batch to be the inputs.\n",
    "\n",
    "--> For each sample x_i in inputs with k features, \n",
    "we compute the mean and variance of the sample:\n",
    "\n",
    "-> python mean_i = sum(x_i[j] for j in range(k)) / k \n",
    "\n",
    "-> var_i = sum((x_i[j] - mean_i) ** 2 for j in range(k)) / k\n",
    "\n",
    "--> and then compute a normalized x_i_normalized, including a small factor epsilon for numerical stability.\n",
    "\n",
    "-> python x_i_normalized = (x_i - mean_i) / sqrt(var_i + epsilon)\n",
    "\n",
    "--> And finally x_i_normalized is linearly transformed by gamma and beta, which are learned parameters:\n",
    "\n",
    "-> python output_i = x_i_normalized * gamma + beta\n",
    "\n",
    "gamma and beta will span the axes of inputs specified in axis, and this part of the inputs' shape must be fully defined.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#note\n",
    "\n",
    "\"\"\"\n",
    "Note that other implementations of layer normalization may choose to define gamma and beta over a separate set of axes from the axes being normalized across. \n",
    "\n",
    "For example, Group Normalization (Wu et al. 2018) with group size of 1 corresponds to a Layer Normalization that normalizes across height, width, and channel and has gamma and beta span only the channel dimension.\n",
    "\n",
    "So, this Layer Normalization implementation will not match a Group Normalization layer with group size set to 1.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input shape Arbitrary. Use the keyword argument input_shape (tuple of\n",
    "\n",
    "#integers, does not include the samples axis) when using this layer as the first layer in a model.\n",
    "\n",
    "#Output shape Same shape as input\n",
    "\n",
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
